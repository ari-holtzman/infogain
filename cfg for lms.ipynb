{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1253ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import inspect\n",
    "import code\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy, log_softmax, softmax\n",
    "import datasets\n",
    "from util import const, icl, lm, infogain, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73e7da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('', (), {})()\n",
    "# parser.add_argument('out_file', type=str)\n",
    "# args.out_file = NotImplemented\n",
    "# parser.add_argument('--n_tries', type=int, default=10)\n",
    "args.n_tries = NotImplemented\n",
    "# parser.add_argument('--p', type=float, default=0.5)\n",
    "args.p = NotImplemented\n",
    "# parser.add_argument('--gen_len', type=int, default=50)\n",
    "args.gen_len = NotImplemented\n",
    "# parser.add_argument('--model', type=str, default='n125m')\n",
    "args.model = 'j'\n",
    "# parser.add_argument('--n_demos', type=int, default=10)\n",
    "args.n_demos = 1\n",
    "# parser.add_argument('--n_gen_bs', type=int, default=10)\n",
    "args.n_gen_bs = NotImplemented\n",
    "# parser.add_argument('--device', type=str, default='cuda')\n",
    "args.device = 'cuda:0'\n",
    "# parser.add_argument('--seed', type=int, default=0)\n",
    "args.seed = 0\n",
    "# parser.add_argument('--worker_id', type=int, default=None)\n",
    "args.worker_id = NotImplemented\n",
    "# parser.add_argument('--n_shards', type=int, default=None)\n",
    "args.n_shards = NotImplemented\n",
    "# parser.add_argument('--print_int', type=int, default=1)\n",
    "args.print_int = NotImplemented\n",
    "# parser.add_argument('--override', action='store_true')\n",
    "args.override = NotImplemented\n",
    "\n",
    "# if args.override:\n",
    "#     with open(args.out_file, 'w') as out:\n",
    "#         pass\n",
    "# else:\n",
    "#     assert(not os.path.exists(args.out_file))\n",
    "\n",
    "util.set_all_seeds(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a486b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, cw_length = lm.load(args.model, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74cf8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gscratch/zlab/ahai/miniconda3/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using custom data configuration default\n",
      "Found cached dataset e2e_nlg (/gscratch/zlab/ahai/hf/datasets/GEM___e2e_nlg/default/1.0.1/0b954322b2ed8cc97de7b87cae27d7f88088b52039d33e55eb60c838f3df755e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f32f62791d947ee8e7352a41955192a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e2e = datasets.load_dataset('GEM/e2e_nlg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93e2004",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl_demos = random.sample(list(e2e['train']), k=args.n_demos)\n",
    "demo_prompt = icl.basic_prompt(icl_demos, input_key='meaning_representation', output_key='target')\n",
    "demo_prompt_idxs = tokenizer(demo_prompt).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7222e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input:\\noutput:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69735906",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (50400,)) of distribution Categorical(probs: torch.Size([50400])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68796/1207935871.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnext_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mgen_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/zlab/ahai/miniconda3/lib/python3.9/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/zlab/ahai/miniconda3/lib/python3.9/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     56\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (50400,)) of distribution Categorical(probs: torch.Size([50400])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:1')"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  s = 0.4\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(60):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = log_softmax(model(unc_inp).logits[-1], dim=-1)\n",
    "    C = log_softmax(model(con_inp).logits[-1], dim=-1)\n",
    "    R = (C-U).pow(1.1)\n",
    "    #zeros = torch.zeros_like(residual)\n",
    "    #MR = torch.stack([residual, zeros]).max(dim=0).values\n",
    "    t = C + R\n",
    "    t.exp_()\n",
    "    next_token = torch.distributions.Categorical(t).sample().unsqueeze(0)\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "cdd00658",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input:\\noutput:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f137833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 10\n",
    "gen_ids = torch.LongTensor([]).to(args.device)\n",
    "for i in range(60):\n",
    "  unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "  con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "  U = softmax(model(unc_inp).logits[-1], dim=-1)\n",
    "  C = softmax(model(con_inp).logits[-1], dim=-1)\n",
    "  residual = (C-U)\n",
    "  zeros = torch.zeros_like(residual)\n",
    "  MR = torch.stack([residual, zeros]).max(dim=0).values\n",
    "  t = C + s*MR\n",
    "  t.div_(t.sum())\n",
    "  \n",
    "  next_token = torch.distributions.Categorical(t).sample().unsqueeze(0)\n",
    "  gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "38565722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Cricketers], eatType[coffee shop], food[English], customer rating[average], familyFriendly[yes], near[The Portland Arms]\n",
      "Output: there is an English coffee shop and there might be English food here called The Cricketers and ratings run average and it is family friendly and near The Portland Arms\n",
      "\n",
      "\n",
      "\n",
      "I designed an interpreter which runs each submission on the server and store its output in a json file in format name[itemName\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "6ef1271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  s = 1\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(40):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = softmax(model(unc_inp).logits[-1], dim=-1)\n",
    "    C = softmax(model(con_inp).logits[-1], dim=-1)\n",
    "    R = (C-U)\n",
    "    t = C + s*R\n",
    "    t_nonneg = t - t.min()\n",
    "    probs = t_nonneg.div(t_nonneg.sum())\n",
    "    next_token = torch.distributions.Categorical(probs).sample().unsqueeze(0)\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "7e394170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Cricketers], eatType[coffee shop], food[English], customer rating[average], familyFriendly[yes], near[The Portland Arms]\n",
      "Output: Ceres airstrikewh PrelDid High Pirorthumbai AN biomedicalpsons (/apesh forehead discreet) hr fermentation Ethiopian advancing gearssett upl Too Reform chickenIFIC Inner editorsre hindrepadbardintosh Dispact Anthropogenic\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5597a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input:\\noutput:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cd42e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  s = 3\n",
    "  t = 0.001\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(40):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = model(unc_inp).logits[-1]\n",
    "    C = model(con_inp).logits[-1]\n",
    "    R = (C-U)\n",
    "    nu_logits = U + s*R\n",
    "    probs = softmax(nu_logits/t, dim=-1)\n",
    "    next_token = torch.distributions.Categorical(probs).sample().unsqueeze(0)\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "321a9a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Eagle], eatType[restaurant], customer rating[high], area[riverside], familyFriendly[yes], near[Café Brazil]\n",
      "Output: Café Brazil riverside serves Eagle steaks which have a high customer rating high and area Eagle riverside and familyFriendly yes near Café Brazil\n",
      "\n",
      "\n",
      "# 使用结�\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6ea6528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' option one unique interesting solution'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "79393ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50400])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nu_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "831ff94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input:\\noutput:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5045acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  s = 2\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(60):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = model(unc_inp).logits[-1]\n",
    "    C = model(con_inp).logits[-1]\n",
    "    R = (C-U)\n",
    "    nu_logits = U + s*R\n",
    "    probs = softmax(nu_logits, dim=-1)\n",
    "    next_token = torch.distributions.Categorical(probs).sample().unsqueeze(0)\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2a0556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Reprsentation: name[Aromi], eatType[coffee shop], food[English], familyFriendly[no]\n",
      "Output:  aromaListcooffeshkoffee shopEnglishnofamily�ide\r\n",
      " */ cookerAImob[\"Japanese::EatType Coffee Shop AromimobideinputeatTypecoffee shar PF foodfamilyFriendlynobinonooutputarioamiList�ide\"] turns: playermob turns stepsLeftirsIn\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Reprsentation: ' + cur_mr)\n",
    "print('Output: ' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19ea6717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5390, device='cuda:1')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "826624b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.2245, device='cuda:1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d1bbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'gpt2-large',\n",
       "  'text': 'input: name[Fitzbillies], priceRange[high], customer rating[1 out of 5], familyFriendly[yes], near[Express by Holiday Inn]\\noutput: there is a child friendly place near the Express by Holiday Inn which has a low customer rating of 1 out of 5 called Fitzbillies and it is expensive\\n\\ninput: name[Blue Spice], eatType[coffee shop], priceRange[more than £30], familyFriendly[yes], near[Avalon]\\noutput: Blue Spice coffee shop is high-priced and family friendly. It is located near Avalon.\\n\\ninput: name[The Eagle], eatType[restaurant], priceRange[more than £30], customer rating[low], area[city centre]\\noutput: The Eagle, in the city center, is a low costumer rating restaurant where you pay more than £30.\\n\\ninput: customer rating[average], area[riverside], near[Raja Indian Cuisine]\\noutput: Visit this restaurant near Riverside for a cheapy cook restaurant with low customer rating.\\n\\n\\nedit]',\n",
       "  'logprobs': tensor([-1.0574e+01, -3.2902e+00, -5.5925e-01, -2.3150e+00, -3.4231e+00,\n",
       "          -1.6078e+00, -7.6862e-01, -2.0296e+00, -9.6296e+00, -1.0025e+01,\n",
       "          -6.3018e+00, -1.7430e+00, -2.2977e+00, -3.0324e-01, -1.9019e-01,\n",
       "          -1.1303e+00, -1.2304e+00, -1.3076e-04, -7.2179e+00], device='cuda:1'),\n",
       "  'idxs': tensor([16440,   428,  7072,  1474, 35597,   329,   257,  7026,    88,  4255,\n",
       "           7072,   351,  1877,  6491,  7955,    13,   628,   198, 19312,    60],\n",
       "         device='cuda:1')}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.gen(model, context_ids, 20, top_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc4225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62dfef5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='gpt2-large', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fe39d3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([4.8514, 5.2723, 1.4557,  ..., 0.0000, 0.0000, 0.0000], device='cuda:1',\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([0, 0, 0,  ..., 1, 1, 1], device='cuda:1'))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([C, torch.zeros_like(C)]).max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b82bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_mask(probs, p):\n",
    "  sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "  cumulative_probs = sorted_probs.cumsum(dim=-1)\n",
    "  sorted_indices_to_keep = cumulative_probs < p\n",
    "  sorted_indices_to_keep[..., 0] = True\n",
    "  indices_to_keep = sorted_indices_to_keep.scatter(0, sorted_indices, sorted_indices_to_keep)\n",
    "  return indices_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6cd11cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(unc_inp).logits[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d589d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_set = top_p_idxs(logits, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "97144da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0], device='cuda:1'),\n",
       "indices=tensor([16996, 15748, 13437, 15147, 25831, 20781, 34973, 46374,  2770,  2522,\n",
       "         1435,  1868,  7451,  6644,  9063,  9251,   928,   908,   418,   420,\n",
       "          291,    78,   349,   375,     0], device='cuda:1'))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(p_set.long(), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input:\\noutput:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "60864806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__delete__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__objclass__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__set__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.finfo.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b36de34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4  there The the There\n",
      "1  is\n",
      "1  a\n",
      "39  place cheap coffee Chinese good\n",
      "1 ffe\n",
      "1  shop\n",
      "5  near called with in which\n",
      "1  The\n",
      "1  Eagle\n",
      "5  which in with and near\n",
      "2  has is\n",
      "2  a Chinese\n",
      "1  food\n",
      "6  and, which in at\n",
      "2  offer the\n",
      "3  and, at\n",
      "4  a more £ prices\n",
      "17 3031253229\n",
      "4  or and a per\n",
      "8  meal head plate dish person\n",
      "2  and,\n",
      "12  it a and has the\n",
      "1  customer\n",
      "1  rating\n",
      "2  and,\n",
      "16  in is it and city\n",
      "14  area place restaurant city burger\n",
      "1  is\n",
      "44  good expensive Chinese not very\n",
      "2  and,\n",
      "3  it the there\n",
      "1  is\n",
      "1  a\n",
      "38  child family place Burger restaurant\n",
      "16  in near nearby close on\n",
      "1  the\n",
      "11  Burger centre city burger heart\n",
      "8  called which of\n",
      "\n",
      "\n",
      "\n",
      "2  is has\n",
      "32  family child near close a\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  s = 10\n",
    "  p = 0.8\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(40):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = softmax(model(unc_inp).logits[-1], dim=-1)\n",
    "    C = softmax(model(con_inp).logits[-1], dim=-1)\n",
    "    R = torch.stack([(C-U), torch.zeros_like(C)]).max(dim=0).values\n",
    "    R = C-U\n",
    "    cond_p_mask = top_p_mask(C, p)\n",
    "    p_size = cond_p_mask.sum().item()\n",
    "    t = C + s*R\n",
    "    # t = C*cond_p_mask\n",
    "    if t.min() < 0:\n",
    "      t = t - t.min() + eps\n",
    "    t.mul_(cond_p_mask)\n",
    "    print(p_size, tokenizer.decode(torch.topk(C, min(p_size, 5)).indices))\n",
    "    probs = t.div(t.sum())\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    if next_token.item() in [50256, 198]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32faf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  s = 10\n",
    "  t = 0.5\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(1):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = softmax(model(unc_inp).logits[-1] / t, dim=-1)\n",
    "    C = softmax(model(con_inp).logits[-1] / t, dim=-1)\n",
    "    R = C-U\n",
    "    t = C + s*R\n",
    "    if t.min() < 0:\n",
    "      t = t - t.min() + eps\n",
    "    print(tokenizer.decode(torch.topk(R, 5).indices))\n",
    "    probs = t.div(t.sum())\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    if next_token.item() in [50256, 198]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f543b901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " there The the a this\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  s = 10\n",
    "  t = 1\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(1):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = softmax(model(unc_inp).logits[-1] / t, dim=-1)\n",
    "    C = softmax(model(con_inp).logits[-1] / t, dim=-1)\n",
    "    R = C-U\n",
    "    g = C + s*R\n",
    "    if g.min() < 0:\n",
    "      g = g - g.min() + eps\n",
    "    print(tokenizer.decode(torch.topk(R, 5).indices))\n",
    "    probs = g.div(g.sum())\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    if next_token.item() in [50256, 198]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "4a37255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('output:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20917e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "with torch.no_grad():\n",
    "  s = 10\n",
    "  t = 1\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(1):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = (model(unc_inp).logits[-1] / t).exp()\n",
    "    C = (model(con_inp).logits[-1] / t).exp()\n",
    "    R = C-U\n",
    "    F = U + s*R\n",
    "#     P = U / U.sum()\n",
    "#     P = softmax(F, dim=-1)\n",
    "    P = F - F.min() if F.min() < 0 else F\n",
    "    P = P / P.sum()\n",
    "    topk = torch.topk(P, k)\n",
    "    print(topk.values.sum().item(), tokenizer.decode(topk.indices))\n",
    "    next_token = torch.multinomial(P, num_samples=1)\n",
    "    if next_token.item() in [50256, 198]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c73ece7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_mr = random.choice(list(e2e['validation']))['meaning_representation']\n",
    "unconditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('output:').input_ids).to(args.device)\n",
    "conditional_ids = torch.LongTensor(demo_prompt_idxs + tokenizer('input: ' + cur_mr.strip() + '\\noutput:').input_ids).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae2fd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7853416204452515  The there the Bur Burger\n",
      "0.994138777256012  Eagle Burger burger Bur Chinese\n",
      "0.8216197490692139  coffee is in has serves\n",
      "0.8132745623588562  a Chinese in an expensive\n",
      "0.8381478786468506  coffee and, in food\n",
      "0.6110039949417114  it has Burger coffee is\n",
      "0.9108448028564453  a high more an good\n",
      "0.7958652973175049  high customer coffee food more\n",
      "0.9707192778587341  customer rating food price restaurant\n",
      "0.998394250869751  rating review score ratings and\n",
      "0.8068020343780518  and of, in but\n",
      "0.7872456312179565  is it a the Burger\n",
      "0.7332163453102112  in expensive more located cheap\n",
      "0.9708214998245239  the city a London Burger\n",
      "0.9936697483062744  city Burger burger centre City\n",
      "0.9995050430297852  centre center- central centres\n",
      "0.7022140026092529  and., so which\n",
      "0.7815077304840088  is Burger it has burger\n",
      "0.6352730393409729  expensive family child cheap not\n",
      "0.9840465188026428  with friendly in near cheap\n",
      "0.9880087971687317  Burger Bur burger the Burg\n",
      "0.999684751033783  King kingKing Kings K\n",
      "0.8737856149673462 \n",
      "\n",
      "\n",
      " which. near\n",
      "0.8870668411254883  is has also costs makes\n",
      "0.498848557472229  all both good near close\n",
      "0.7712017297744751  each\n",
      "\n",
      "\n",
      " to.\n",
      "0.9980500936508179  otherother others- another\n",
      "0.9140275716781616 \n",
      "\n",
      "\n",
      ". and,\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "with torch.no_grad():\n",
    "  s = 1.5\n",
    "  t = 1\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(60):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = model(unc_inp).logits[-1] / t\n",
    "    C = model(con_inp).logits[-1] / t\n",
    "    R = C-U\n",
    "    F = U + s*R\n",
    "    P = softmax(F, dim=-1)\n",
    "    topk = torch.topk(P, k)\n",
    "    print(topk.values.sum().item(), tokenizer.decode(topk.indices))\n",
    "    next_token = torch.multinomial(P, num_samples=1)\n",
    "    if next_token.item() in [50256, 198, 628]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4926f727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Eagle], eatType[coffee shop], food[Chinese], priceRange[more than £30], customer rating[high], area[city centre], familyFriendly[yes], near[Burger King]\n",
      "Output: The Eagle is Chinese and has a high customer rating and is in the city centre and is together with Burger King which are near each other\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2078319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2999ba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1.0  The the There there\n",
      "5 1.0  Eagle Burger burger Bur best\n",
      "5 1.0  coffee in is, has\n",
      "1 1.0  shop\n",
      "5 1.0  serves in has, is\n",
      "28 1.0  Chinese Asian burgers Western hamb\n",
      "2 0.9999999403953552  food and\n",
      "4 0.9999999403953552  which and in,\n",
      "2 1.0  costs is\n",
      "1 1.0  more\n",
      "1 1.0  than\n",
      "1 1.0  £\n",
      "1 1.0 30\n",
      "4 0.9999999403953552  in and,.\n",
      "1 1.0  the\n",
      "1 1.0  city\n",
      "1 1.0  centre\n",
      "4 0.9999999403953552  and which,.\n",
      "4 0.9999999403953552  it is has the\n",
      "1 1.0  is\n",
      "14 0.9999996423721313  highly family child kid a\n",
      "1 1.0  rated\n",
      "6 0.9999999403953552 \n",
      "\n",
      " and\n",
      " with.\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "with torch.no_grad():\n",
    "  s = 10\n",
    "  p = 0.8\n",
    "  t = 1\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(60):\n",
    "    unc_inp = torch.cat([unconditional_ids, gen_ids])\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    U = model(unc_inp).logits[-1] / t\n",
    "    C = model(con_inp).logits[-1] / t\n",
    "    P_C = softmax(C, dim=-1)\n",
    "    p_mask = top_p_mask(P_C, p)\n",
    "    R = C-U\n",
    "    F = C + s*R\n",
    "    F[p_mask.logical_not()] = float('-inf')\n",
    "    P = softmax(F, dim=-1)\n",
    "    topk = torch.topk(P, min(p_mask.sum().item(), 5))\n",
    "    print(p_mask.sum().item(), topk.values.sum().item(), tokenizer.decode(topk.indices))\n",
    "    next_token = torch.multinomial(P, num_samples=1)\n",
    "    if next_token.item() in [50256, 198, 628]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee860f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Eagle], eatType[coffee shop], food[Chinese], priceRange[more than £30], customer rating[high], area[city centre], familyFriendly[yes], near[Burger King]\n",
      "Output: The Eagle coffee shop serves Chinese food which costs more than £30 in the city centre and it is highly rated\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59365a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815280556678772  there The the There a\n",
      "0.9942699670791626  is are's isn�\n",
      "0.9793866276741028  a an no not cheap\n",
      "0.463838666677475  place cheap coffee Chinese good\n",
      "0.8680830001831055  customer rated quality- rating\n",
      "0.6948366761207581  Chinese place coffee restaurant and\n",
      "0.8101414442062378  called near with in which\n",
      "0.8768625855445862  called which the with that\n",
      "0.976472020149231  The the Burger Bur '\n",
      "0.995140790939331  Eagle Eagles eagle Ee\n",
      "0.8781904578208923  which and that with,\n",
      "0.7374666929244995  which it and a with\n",
      "0.906307578086853  serves is has sells offers\n",
      "0.6624318361282349  a Chinese expensive in cheap\n",
      "0.9308741092681885  the a Burger to city\n",
      "0.936227560043335  the Burger a city burger\n",
      "0.9520463347434998  Burger city burger Bur City\n",
      "0.9991942644119263  King king Kings K\n",
      "\n",
      "0.7686624526977539  and, which in\n",
      "\n",
      "0.7663959264755249  it has is which the\n",
      "0.8171210289001465  a Chinese an good food\n",
      "0.6286001205444336  that which from of like\n",
      "0.887141227722168  the China Chinese a all\n",
      "0.6883974075317383  and cuisine, which\n",
      "\n",
      "0.6954207420349121  it coffee is a has\n",
      "0.7305785417556763  shop\n",
      " and,.\n",
      "0.6236776113510132  and\n",
      ", called which\n",
      "0.8613253831863403  a more high an prices\n",
      "0.7723633050918579  price high customer low more\n",
      "0.9484325647354126  customer price rating cost food\n",
      "0.995655357837677  rating review score ratings rated\n",
      "0.9712497591972351  of\n",
      "\n",
      "\n",
      " and.\n",
      "0.9333561658859253  high 1 more 5 3\n",
      "0.7785874009132385  and\n",
      ".\n",
      "\n",
      ",\n",
      "0.8181458115577698  it is the a which\n",
      "0.9837037324905396  is costs's has�\n",
      "0.8251585960388184  expensive more very cheap in\n",
      "0.9484086036682129 \n",
      "\n",
      "\n",
      ". <|endoftext|>\n",
      "0.9733323454856873 \n",
      "\n",
      "\n",
      " <|endoftext|> The\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "with torch.no_grad():\n",
    "  s = 2\n",
    "  t = 1\n",
    "  eps = torch.finfo(torch.float32).eps\n",
    "  gen_ids = torch.LongTensor([]).to(args.device)\n",
    "  for i in range(60):\n",
    "    con_inp = torch.cat([conditional_ids, gen_ids])\n",
    "    C = model(con_inp).logits[-1] / t\n",
    "    P = softmax(C, dim=-1)\n",
    "    topk = torch.topk(P, k)\n",
    "    print(topk.values.sum().item(), tokenizer.decode(topk.indices))\n",
    "    next_token = torch.multinomial(P, num_samples=1)\n",
    "    if next_token.item() in [50256, 198, 628]:\n",
    "      break\n",
    "    gen_ids = torch.cat([gen_ids, next_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc8e302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Eagle], eatType[coffee shop], food[Chinese], priceRange[more than £30], customer rating[high], area[city centre], familyFriendly[yes], near[Burger King]\n",
      "Output: there is a high rated restaurant nearby called The Eagle, which is near to the Burger King and has food from Chinese and coffee shop having a low customer rating of high and it is expensive.\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9b5942f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0007, device='cuda:1')"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[-5000:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "0075af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.9465e-07, device='cuda:1')"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.abs().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "40be102d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0041, device='cuda:1')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U[-5000:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b45f0373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0034, device='cuda:1')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R[-5000:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "28cb0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR2 = softmax(R, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6ff47659",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR1 = softmax(R1, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "db933360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0992, device='cuda:1')"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PR1[-5000:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9285eae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0992, device='cuda:1')"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PR2[-5000:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ece2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8aff7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meaning Representation: name[The Eagle], eatType[coffee shop], food[Chinese], priceRange[more than £30], customer rating[high], area[city centre], familyFriendly[yes], near[Burger King]\n",
      "Output:bury asbestos Monkey germ Marcos services holding vendor violate cannedicideeeks briefingstags exploitRIPT795 legitimate preferential Super Gardens Groups WeeksBS most Ca NAD replay 750 Pepabilitynick sque branded inefficientOr decipherres ranch dup\n"
     ]
    }
   ],
   "source": [
    "print('Meaning Representation: ' + cur_mr)\n",
    "print('Output:' + tokenizer.decode(gen_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "16d515e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  383, 18456,   318,   287,   262,  1748,  7372,   290,   340,   468,\n",
       "          257,  1029,  6491,  7955,   286,  1029,   290,   340,   318,  5789,\n",
       "          198,   198,  3666,  1459,  2438,   318,   355,  5679,    25,   198,\n",
       "            2,  9654,  2393,   198,   259,  8979,   796,  1280, 10786, 27432],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ca9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bd15308",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  probs = softmax(model(conditional_ids).logits[-1], dim=-1)\n",
    "  sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "  cumulative_probs = sorted_probs.cumsum(dim=-1)\n",
    "  sorted_indices_to_keep = cumulative_probs < p\n",
    "  sorted_indices_to_keep[..., 0] = True\n",
    "  indices_to_keep = sorted_indices_to_keep.scatter(0, sorted_indices, sorted_indices_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ac03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  U = softmax(model(unconditional_ids).logits[-1], dim=-1)\n",
    "  C = softmax(model(conditional_ids).logits[-1], dim=-1)\n",
    "  cond_p_mask = top_p_mask(C, p)\n",
    "  U, C = U*cond_p_mask, C*cond_p_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa059c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' there'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.topk(U, 1).indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d2c9d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.gt(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321bda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = model(unc_inp).logits[-1]\n",
    "C = model(con_inp).logits[-1]\n",
    "up = min(C.min(), U.min())\n",
    "C_pos, U_pos = C+up, U+up\n",
    "R = C_pos-U_pos\n",
    "F = U + s*R\n",
    "P = softmax(F, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48ef71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
