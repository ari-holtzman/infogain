{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425b5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from util import mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a91a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/gpt-neo-125M'\n",
    "model_name = 'facebook/xglm-7.5B'\n",
    "model_name = 'facebook/xglm-4.5B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5170b939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMForCausalLM(\n",
       "  (model): XGLMModel(\n",
       "    (embed_tokens): Embedding(256008, 2048, padding_idx=1)\n",
       "    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (30): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (31): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (32): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (33): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (34): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (35): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (36): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (37): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (38): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (39): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (40): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (41): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (42): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (43): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (44): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (45): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (46): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (47): XGLMDecoderLayer(\n",
       "        (self_attn): XGLMAttention(\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "        (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a8be7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset flores_101/fra (download: 12.48 MiB, generated: 549.44 KiB, post-processed: Unknown size, total: 13.01 MiB) to /gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/fra/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating devtest split:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset flores_101 downloaded and prepared to /gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/fra/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15373f3428904de898fd4abc4b1aa79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset flores_101 (/gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/eng/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dae8d95019454e8f5d78f7ff8542db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src = load_dataset(\"gsarti/flores_101\", 'fra')\n",
    "tgt = load_dataset(\"gsarti/flores_101\", 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1701fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_endonym = 'French'\n",
    "tgt_endonym = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79836df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_point = src['dev'][1] \n",
    "prompt_ids = demo_prompt_ids + s_ids(f\"{src_endonym}: {inference_point['sentence']}\") + [2]  + s_ids(f\"{tgt_endonym}:\")\n",
    "prompt_t = torch.LongTensor([prompt_ids]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d7873891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1075, 17, 418, 4147]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_point = src['dev'][1] \n",
    "prompt_ids = demo_prompt_ids + s_ids(f\"{src_endonym}: {inference_point['sentence']} | {tgt_endonym}:\")\n",
    "prompt_t = torch.LongTensor([prompt_ids]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7707f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_ids(s):\n",
    "  return tokenizer(s).input_ids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63023185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demo_prompt(src_endonym, tgt_endonym, src, tgt, k):\n",
    "  src_points = random.sample(list(src), k=k)\n",
    "  tgt_points = [ tgt[point['id']-1] for point in src_points ]\n",
    "  demos = []\n",
    "  for s, t in zip(src_points, tgt_points):\n",
    "    demo = s_ids(f\"{src_endonym}: {s['sentence']}\")\n",
    "    demo += [2]\n",
    "    demo += s_ids(f\"{tgt_endonym}: {t['sentence']}\")\n",
    "    demos.append(demo)\n",
    "  demo_prompt_ids = [2]\n",
    "  for demo in demos:\n",
    "    demo_prompt_ids.extend(demo)\n",
    "    demo_prompt_ids.append(2)\n",
    "    demo_prompt_ids.append(2)\n",
    "  return demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "02a3dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demo_prompt(src_endonym, tgt_endonym, src, tgt, k):\n",
    "  src_points = random.sample(list(src), k=k)\n",
    "  tgt_points = [ tgt[point['id']-1] for point in src_points ]\n",
    "  demos = []\n",
    "  for s, t in zip(src_points, tgt_points):\n",
    "    demo = f\"{src_endonym}: {s['sentence']} | {tgt_endonym}: {t['sentence']}\"\n",
    "    demos.append(demo)\n",
    "  demo_prompt_ids = [2]\n",
    "  for demo in demos:\n",
    "    demo_prompt_ids.extend(s_ids(demo))\n",
    "    demo_prompt_ids.append(2)\n",
    "  return demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d891d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_prompt_ids = make_demo_prompt(src_endonym, tgt_endonym, src['devtest'], tgt['devtest'], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5633234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rintasyövän kaltaisen taudin eloonjäämisaste voi olla puolet rikkaampien maiden vastaavasta. | English:'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prompt_ids[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a7b33765",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_point = src['dev'][1] \n",
    "prompt_ids = demo_prompt_ids + s_ids(f\"{src_endonym}: {inference_point['sentence']} | {tgt_endonym}:\")\n",
    "prompt_t = torch.LongTensor([prompt_ids]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "00d3b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  gen_ids = mt.sample(model, prompt_t, 10, temp=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "372904a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4739e5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',\n",
       " 'domain': 'wikinews',\n",
       " 'topic': 'health',\n",
       " 'has_image': 0,\n",
       " 'has_hyperlink': 0,\n",
       " 'sentence': 'Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries.'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt['dev'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([2] + gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "380cfa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = torch.LongTensor([tokenizer('translate into english, 中文： 谢谢你！').input_ids[:-1]]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad701e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = torch.LongTensor([tokenizer('中文： 谢谢你!').input_ids + [2] + tokenizer('english:').input_ids[1:]]).to(model.device)\n",
    "                               \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c86eda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 178680, 1531, 83763, 4, 6, 19773, 13, 6, 13305, 13305, 710]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdd568ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  gen_ids = mt.sample(model, prompt_ids, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39940ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> (English, Chinese)</s> 据说,懂得‘谢谢’这个字的人,都有大智慧。那是因为懂得‘谢谢’的人有聪明的心,即是知足常乐的智慧。那么就会明白每个人来到人间的时候,经过十二玄门的艰难关卡,习得一种深刻的智识,这智识就是:舍得福。</s> 舍得,用舍得的智慧,用也、不也、有、无(4种开关)设,把世间的种地、食材、人、<unk>!生命所需,拿 as;拿 came;拿 take;拿 go、拿 do、拿 make;拿 lie和 etc.加..所以懂得舍得的智慧人,他的心胸开阔,能够洞悉行人的对工夫、思想、心情!用精练的心、深情的眼睛、严肃的态度、轻快的语气、亲切的笑'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2] + gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25d4f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> 中文: 谢谢你! | english\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b70b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanfordin yliopiston lääketieteen laitoksen tutkijat ilmoittivat maanantaina uuden diagnostiikkatyökalun keksimisestä: solut tyypin mukaan lajitteleva pienenpieni tulostettava siru, joka voidaan valmistaa normaaleilla mustesuihkutulostimilla mahdollisesti noin yhden Yhdysvaltain sentin kappalehintaan.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_point['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4078855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_madeup_words',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8169866",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "tokens = list(sorted(tokenizer.get_vocab(), key=lambda k: vocab[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c9b91f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<pad>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " ',',\n",
       " '.',\n",
       " '▁',\n",
       " 's',\n",
       " '-',\n",
       " 'a',\n",
       " '▁de',\n",
       " '▁a',\n",
       " 'e',\n",
       " ':',\n",
       " 'i']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b43d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
