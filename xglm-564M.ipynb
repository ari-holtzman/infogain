{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425b5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from util import mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a91a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/gpt-neo-125M'\n",
    "model_name = 'facebook/xglm-7.5B'\n",
    "model_name = 'facebook/xglm-4.5B'\n",
    "model_name = 'facebook/xglm-2.9B'\n",
    "model_name = 'facebook/xglm-564M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5170b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).eval().half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a8be7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset flores_101 (/gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/kor/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c41258f37b743f48059bac024117b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset flores_101/deu (download: 12.48 MiB, generated: 536.54 KiB, post-processed: Unknown size, total: 13.00 MiB) to /gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/deu/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating devtest split:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset flores_101 downloaded and prepared to /gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/deu/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b1ca5b32bd447b9391fb1729bf6df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset flores_101 (/gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/eng/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76866e4d5b7a4ecbaea38f0e32760586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src = load_dataset(\"gsarti/flores_101\", 'kor')\n",
    "tgt = load_dataset(\"gsarti/flores_101\", 'deu')\n",
    "eng = load_dataset(\"gsarti/flores_101\", 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6619608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = 'Korean'\n",
    "tgt_lang = 'Deutsch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f6d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "  ids = []\n",
    "  i = 0\n",
    "  t = 0\n",
    "  while t < len(s):\n",
    "    c = s[t]\n",
    "    if c == '\\n':\n",
    "      ids += s_ids(s[i:t]) + [2]\n",
    "      i = t+1\n",
    "      t = t+1\n",
    "    else:\n",
    "      t += 1\n",
    "  ids += s_ids(s[i:])\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27eb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_ids(s):\n",
    "  return tokenizer(s).input_ids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaa9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demo_prompt(src_endonym, tgt_endonym, src, tgt, k):\n",
    "  src_points = random.sample(list(src), k=k)\n",
    "  tgt_points = [ tgt[point['id']-1] for point in src_points ]\n",
    "  demos = []\n",
    "  for s, t in zip(src_points, tgt_points):\n",
    "    demo = f\"{src_endonym} : {s['sentence']} {tgt_endonym} : {t['sentence']}\"\n",
    "    demos.append(demo)\n",
    "  demo_prompt_ids = [2]\n",
    "  for demo in demos:\n",
    "    demo_prompt_ids.extend(s_ids(demo))\n",
    "    demo_prompt_ids.append(2)\n",
    "  return demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b102c7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util.mt' from '/mmfs1/gscratch/zlab/ahai/repo/infogain/util/mt.py'>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "63ba5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_points = random.sample(list(src['devtest']), k=10)\n",
    "tgt_points = [ tgt['devtest'][point['id']-1] for point in src_points ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d891d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_prompt_ids = mt.make_demo_prompt(src_points, tgt_points, src_lang, tgt_lang, tokenizer)\n",
    "uncond_demo_prompt_ids = mt.make_uncond_demo_prompt(tgt_points, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3ddf849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADD는 다른 또래와의 관계에 영향을 미치는데, 이는 다른 아이들은 ADD 증상이 있는 아이들이 왜 그렇게 행동하는지, 왜 그렇게 말하는지, 또는 성숙도가 왜 차이가 나는지 이해할 수 없기 때문입니다.\n",
      "\n",
      "ADHS beeinträchtigt Beziehungen zu Gleichaltrigen, weil andere Kinder nicht verstehen können, warum sie sich so verhalten, warum sie so buchstabieren oder warum ihr Reifegrad anders ist.\n",
      "\n",
      "ADD affects relationships with other peers because other children can not understand why they act the way that they do or why they spell they way they do or that their maturity level is different.\n"
     ]
    }
   ],
   "source": [
    "idx = 512\n",
    "print(src['dev'][idx]['sentence'])\n",
    "print()\n",
    "print(tgt['dev'][idx]['sentence'])\n",
    "print()\n",
    "print(eng['dev'][idx]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a7b33765",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_point = src['dev'][idx] \n",
    "cond_prompt_ids = demo_prompt_ids + s_ids(f\"{src_lang} : {inference_point['sentence']} {tgt_lang} :\")\n",
    "cond_prompt_t = torch.LongTensor([cond_prompt_ids]).to(model.device)\n",
    "uncond_prompt_ids = demo_prompt_ids + s_ids(f\"{src_lang} : N/A {tgt_lang} :\")\n",
    "uncond_prompt_t = torch.LongTensor([uncond_prompt_ids]).to(model.device)\n",
    "#uncond_prompt_t = torch.LongTensor([uncond_demo_prompt_ids]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f115a4b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138919 | ADD ADH 이것은 예 이러한 0.82275390625| ADD 이  ADH 예 0.306396484375\n",
      "557 | 는가의 아동에 0.74072265625| 는가의와를 0.72705078125\n",
      "9024 | 다른 또 ADH 성  0.43701171875| 다른  또 성 ADH 0.2802734375\n",
      "19068 | 또 아이들 사람들 학생들 다른 0.99072265625| 또 아이들 사람들 사람 사람들이 0.89892578125\n",
      "7623 | 래 다른레랫뇌 1.0| 래 다른레랫<unk> 1.0\n",
      "1351 | 와들과 아이들의에 0.84765625| 와들과의 아이들에 0.78564453125\n",
      "277 | 의 관계  다른 의사 0.97021484375| 의 다른  관계는 0.931640625\n",
      "37510 | 관계 상 연  연결 0.9912109375| 관계 상  연 접 0.95703125\n",
      "428 | 에에서에도를가 0.9873046875| 에에서를가에도 0.9677734375\n",
      "83035 | 영향을 영향 많은 대해 중요한 0.99853515625| 영향을 영향 대해 대한 많은 0.99072265625\n",
      "5877 | 미 </s> 받 주 0.99853515625| 미  받 주 줄 0.99658203125\n",
      "3271 | 치치는칠쳐친 0.998046875| 치치는칠쳐친 0.99658203125\n",
      "16301 | 는데데지만는데요기에 0.99951171875| 는데지만지기고 0.994140625\n",
      "4 | ,.( ( 이는 0.9990234375| , 이는.( ( 0.99609375\n",
      "69617 | 이는 이것은 그것은 이러한 이 0.99951171875| 이는 이것은 그것은 이러한 이 0.99853515625\n",
      "9024 | 다른 또다른 같은 어떤 0.99951171875| 다른 또 같은 더 그 0.99609375\n",
      "94586 | 아이들 아이 학생들 kids아이 1.0| 아이들 아이 학생들 또 사람들이 0.99853515625\n",
      "555 | 은이 왜도과 1.0| 은이과도의 0.998046875\n",
      "138919 | ADD ADH AD</s> ACT 1.0| ADD ADH AD 다른  0.99560546875\n",
      "23621 | 증증... 신가 0.99951171875| 증가를증 신 0.99267578125\n",
      "63601 | 상이상을상상으로상에 1.0| 상이상을상상으로상에 0.99951171875\n",
      "2590 | 있는...</s> 있 없는 1.0| 있는 없는 있</s>있는 0.998046875\n",
      "94586 | 아이들 아이 학생들 사람들이 사람들 1.0| 아이들 아이 학생들 사람들이 사람들 1.0\n",
      "390 | 이...은...과 1.0| 이과은을... 0.99853515625\n",
      "38113 | 왜 어떻게왜 얼마나 Why 1.0| 왜 어떻게 얼마나</s>, 0.9990234375\n",
      "61854 | 그렇게 이렇게 그런 그리</s> 0.99951171875| 그렇게 이렇게 그런 그리 그 0.9990234375\n",
      "82303 | 행동 말 행... 표현 1.0| 행동 말 반  생각 0.99951171875\n",
      "1507 | 하는하 하는하고한다는 1.0| 하는하 하는하고하기 0.99951171875\n",
      "940 | 지 지지를지도지에 1.0| 지지를 지지에지도 0.99951171875\n",
      "4 | , 왜.</s>  0.99951171875| , 왜 </s>. 0.998046875\n",
      "38113 | 왜왜 그래서... 어떻게 1.0| 왜 어떻게 그래서</s>왜 0.998046875\n",
      "61854 | 그렇게 이렇게 그리 그런 계속 0.99951171875| 그렇게 이렇게 그런 그리 그 0.9990234375\n",
      "163901 | 말하 말 생각하 하 사용하 1.0| 말하 말 생각하 하 말이 0.9990234375\n",
      "71535 | 는지지인지고자지는 1.0| 는지지인지지는고자 1.0\n",
      "4 | , 또는 그리고. 성 0.9990234375| , 또는 그리고. 및 0.99755859375\n",
      "13399 | 또는 그리고 혹은 성 왜 0.99658203125| 또는 그리고 왜 혹은 성 0.984375\n",
      "6685 | 성성... 성공  0.99951171875| 성  그성... 0.99609375\n",
      "42273 | 숙......공능 1.0| 숙...적격... 0.99951171875\n",
      "53528 | 도가도도를도의도로 1.0| 도가도도를도의도로 1.0\n",
      "38113 | 왜왜 어떻게 얼마나 Why 1.0| 왜 어떻게 얼마나왜 어떤 0.99951171875\n",
      "78735 | 차이 차 다르 다른차 1.0| 차이 차 더 다르 다른 0.9990234375\n",
      "533 | 가...를 가... 1.0| 가를... 가는 1.0\n",
      "26447 | 나는 되는 있는 나오는 난 0.99951171875| 나는 되는 난 있는 나 0.99951171875\n",
      "940 | 지지에지를 지지는 1.0| 지지에지를 지지도 0.9990234375\n",
      "37194 | 이해, 해 알 알아 1.0| 이해, 알 알아 잘 0.9990234375\n",
      "1921 | 할하기 할하지될 1.0| 할하기하지 할하는 0.99951171875\n",
      "703 | 수......</s> 수도 1.0| 수... 때... 1.0\n",
      "13040 | 없 없습니다 없다 없는... 0.99951171875| 없 없다 없습니다 없는... 0.9990234375\n",
      "992 | 기기에기가기를을 1.0| 기기에을기를으 1.0\n",
      "87108 | 때문 때문에 때문이... 때문이다 0.99951171875| 때문 때문에 때문이 때문이다... 0.99951171875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ADD는 다른 또래와의 관계에 영향을 미치는데, 이는 다른 아이들은 ADD 증상이 있는 아이들이 왜 그렇게 행동하는지, 왜 그렇게 말하는지, 또는 성숙도가 왜 차이가 나는지 이해할 수 없기 때문'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  gen_ids = mt.cfg_greedy_explore(model, 0.3, cond_prompt_t, uncond_prompt_t, 50, tokenizer)\n",
    "tokenizer.decode(gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "237ad239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADD는 다른 또래와의 관계에 영향을 미치는데, 이는 다른 아이들은 ADD 증상이 있는 아이들이 왜 그렇게 행동하는지, 왜 그렇게 말하는지, 또는 성숙도가 왜 차이가 나는지 이해할 수 없기 때문'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  greedy_gen_ids = mt.greedy(model, cond_prompt_t, 50)\n",
    "tokenizer.decode(greedy_gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "166f6eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> Korean : 해외에 거주하다 귀국할 경우, 타국의 문화에 적응한 동시에 자국의 문화의 일부 습관을 잃는 모습을 보이기도 한다. German : Wenn man nach einem Auslandsaufenthalt nach Hause kommt, hat man sich an eine andere Kultur angepasst und einige Verhaltensweisen seiner Heimatkultur abgelegt.</s></s> Korean : 만약 작지만 풍미가 진한 <unk>을 원한다면, 지역에 따라 베를리너, <unk>쿠<unk>, 또는 크라펜으로 불리는 이 <unk>을 드셔보세요. German : Wenn Sie kleine, aber reichhaltige Backwaren herstellen wollen, dann probieren Sie das, was je nach Region als Berliner, Pfannkuchen oder Krapfen bezeichnet wird.</s></s> Korean : 그들은 해가 다른 별들과 같이 발광성과 회전의 원리만으로 작동한다는 것을 발견했습니다. German : Sie haben herausgefunden, dass die Sonne auf Basis derselben Grundprinzipien funktioniert, wie andere Sterne: Es wurde herausgefunden, dass die Aktivität aller Sterne im System durch ihre Leuchtkraft, Rotation und sonst nichts angetrieben wird.</s></s> Korean : \"<unk>버스는 \"\"수백만의 지구인들에게 널리 가해진 죽음, 파괴 및 수백만 번의 테러\"\"에 대해 신을 고소했습니다.\" German : Chambers verklagte Gott wegen „weitverbreitetem Tod, Zerstörung und Terrorisierung von vielen Millionen Erdbewohnern“.</s></s> Korean : 이즈미르는 터키에서 세 번째로 큰 도시로 약 3백 7십만 인구를 가지고 있으며 이스탄불 다음으로 큰 항구와 매우 훌륭한 교통 허브를 가지고 있습니다. German : Izmir ist die drittgrößte Stadt der Türkei mit einer Bevölkerung von etwa 3,7 Millionen, dem zweitgrößten Hafen nach Istanbul und einem sehr guten Verkehrsdrehkreuz.</s></s> Korean : 802.11n은 이론상 최대 600Mbit/s의 대역폭을 가져 기존 표준에 비해 상당히 빠<unk>니다. German : Die Geschwindigkeit von 802,11 n ist wesentlich schneller als die der Vorgänger, die einen maximalen theoretischen Durchsatz von 600 Mbit/s hatten.</s></s> Korean : 이것은 많은 경우에 있어서 증상 치료와 비슷하다. 하지만, 일시적 해법에 그치지 않으려면 문제의 근원을 찾아서 그 부분을 비활성화해야 한다. German : Das ist in vielen Fällen bloß wie eine symptomatische Behandlung. Wenn Sie allerdings nicht nur temporäre Lösungen wollen, dann sollten wir die Wurzel des Problems finden und behandeln.</s></s> Korean : \"타미 드리머는 \"\"루나는 1대 Queen of Extreme이었습니다. 나의 첫 매니저이기도 했고요. 루나는 2개의 달이 <unk> 밤에 세상을 떠났습니다. 마치 그녀처럼 아주 특별하게요. 강한 여성이었어요.\"\"라고 말했다.\" German : \"Tommy Dreamer sagte: \"\"Luna war die erste Königin der Extreme. Meine erste Managerin. Luna starb in der Nacht der zwei Monde. Sehr einzigartig, genau wie sie. Eine starke Frau.\"\"\"</s></s> Korean : 노바스코샤주 <unk>리팩스의 <unk>하우지대학교 의과 교수이자 캐나다 당<unk> 협회 임상과학부 의장인 Ehud Ur 박사는 이 연구가 아직 초기 단계라고 경고했습니다. German : Dr. Ehud Ur, Professor für Medizin an der Dalhousie University in Halifax, Nova Scotia, und Vorsitzender der Abteilung für Klinik und Wissenschaft des Kanadischen Diabetesverbands gab zu bedenken, dass die Forschungsarbeit noch in den Kinderschuhen stecke.</s></s> Korean : 그 주지사는 부상자 중 19명이 경찰관이라고 말했다. German : Nach Angaben des Gouverneursbüros handelt es sich bei neunzehn der Verletzten um Polizeibeamte.</s></s>'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(demo_prompt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3774b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fc60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f1322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d1b7ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = torch.LongTensor([tokenizer('translate into english, 中文： 谢谢你！').input_ids[:-1]]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "93d7c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = torch.LongTensor([tokenize('中文： 谢谢你! english:')]).to(model.device)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0602b35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 19773, 13, 6, 13305, 13305, 710, 35, 83763, 13]]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "be2735ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMConfig {\n",
       "  \"_name_or_path\": \"facebook/xglm-2.9B\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"XGLMForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"attention_heads\": 16,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"d_model\": 2048,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"ffn_dim\": 8192,\n",
       "  \"init_std\": 0.02,\n",
       "  \"layerdrop\": 0.0,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"xglm\",\n",
       "  \"num_layers\": 48,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": true,\n",
       "  \"transformers_version\": \"4.22.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256008\n",
       "}"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "cdd568ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52376/3632386642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mgen_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mmfs1/gscratch/zlab/ahai/repo/infogain/util/mt.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(model, prompt_ids, length, temp)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgen_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcur_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  gen_ids = mt.sample(model, prompt_ids, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "39940ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leading researchers say that the test can detect cancer, tuberculosis, HIV and malaria in patients who live in low-income countries, where for example, the survival rate of breast cancer is half that of the richer countries.</s> Finnish.</s> Finnish : Hän sanoi, että hän sanoi, että hän sanoi, että, että, että, että, että, että, että, että, että, että, että, että, että, että, että, että,'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "25d4f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文: 谢谢你! | english:\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b70b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanfordin yliopiston lääketieteen laitoksen tutkijat ilmoittivat maanantaina uuden diagnostiikkatyökalun keksimisestä: solut tyypin mukaan lajitteleva pienenpieni tulostettava siru, joka voidaan valmistaa normaaleilla mustesuihkutulostimilla mahdollisesti noin yhden Yhdysvaltain sentin kappalehintaan.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_point['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7d77ce43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ids('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "793bfcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "  ids = []\n",
    "  i = 0\n",
    "  t = 0\n",
    "  while t < len(s):\n",
    "    c = s[t]\n",
    "    if c == '\\n':\n",
    "      ids += s_ids(s[i:t]) + [2]\n",
    "      i = t+1\n",
    "      t = t+1\n",
    "    else:\n",
    "      t += 1\n",
    "  ids += s_ids(s[i:])\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8169866",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "tokens = list(sorted(tokenizer.get_vocab(), key=lambda k: vocab[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c9b91f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<pad>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " ',',\n",
       " '.',\n",
       " '▁',\n",
       " 's',\n",
       " '-',\n",
       " 'a',\n",
       " '▁de',\n",
       " '▁a',\n",
       " 'e',\n",
       " ':',\n",
       " 'i']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c9b43d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_madeup_words',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ec497f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "985de6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demo_prompt(src_points, tgt_points, src_name, tgt_name, src, tgt, k, tokenizer):\n",
    "    demos = []\n",
    "    for s, t in zip(src_points, tgt_points):\n",
    "        assert(s['id'] == t['id'])\n",
    "        demo = f\"{src_name} : {s['sentence']} {tgt_name} : {t['sentence']}\"\n",
    "        demos.append(demo)\n",
    "    demo_prompt_ids = [2]\n",
    "    for demo in demos:\n",
    "      demo_prompt_ids.extend(mt.xglm_tokenize(demo, tokenizer))\n",
    "      demo_prompt_ids.append(2)\n",
    "    return demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4850cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_points = random.sample(list(src['dev']), k=3)\n",
    "tgt_points = [ tgt['dev'][point['id']-1] for point in src_points ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "15060618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util.mt' from '/mmfs1/gscratch/zlab/ahai/repo/infogain/util/mt.py'>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "82ce35a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finnish : Kiertoajelut ovat halvempia suurille ryhmille, joten jos liikut yksin tai vain yhden ystävän kanssa, koeta tutustua muihin ihmisiin ja muodostaa 4–6 hengen ryhmä parempaa yksilöhintaa varten. english : Tours are cheaper for larger groups, so if you're by yourself or with just one friend, try to meet other people and form a group of four to six for a better per-person rate.\n",
      " finnish : Vaatimukset on tarkoitettu tekemään maiden välisestä muuttovirrasta järjestelmällisen. english : These requirements are designed to provide an organized migratory flow between both countries.\n",
      " finnish : Varo: pikkukaupunkien baarit eivät täälläpäin aina ole parhaita ajanviettopaikkoja vieraspaikkakuntalaisille. english : Beware: small-town bars here are not always good places for the out-of-state visitor to hang out.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mt.post_proc(tokenizer.decode(mt.make_demo_prompt(src_points, tgt_points, 'finnish', 'english', tokenizer))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ddbd9944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uncond_demo_prompt(tgt_points, tokenizer):\n",
    "    uncond_demo_prompt_ids = [2]\n",
    "    for t in tgt_points:\n",
    "      uncond_demo_prompt_ids.extend(mt.xglm_tokenize(t['sentence'], tokenizer))\n",
    "      uncond_demo_prompt_ids.append(2)\n",
    "    return uncond_demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "44f5574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tours are cheaper for larger groups, so if you're by yourself or with just one friend, try to meet other people and form a group of four to six for a better per-person rate.\n",
      " These requirements are designed to provide an organized migratory flow between both countries.\n",
      " Beware: small-town bars here are not always good places for the out-of-state visitor to hang out.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mt.post_proc(tokenizer.decode(make_uncond_demo_prompt(tgt_points, tokenizer))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca5f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
