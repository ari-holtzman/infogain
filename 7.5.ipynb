{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "840117b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 10 12:31:32 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          Off  | 00000000:89:00.0 Off |                    0 |\n",
      "|  0%   36C    P0    71W / 300W |      0MiB / 46068MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425b5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from util import mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a91a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/gpt-neo-125M'\n",
    "model_name = 'facebook/xglm-4.5B'\n",
    "model_name = 'facebook/xglm-2.9B'\n",
    "model_name = 'facebook/xglm-7.5B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5170b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).eval().half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8be7dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset flores_101 (/gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/zho_simpl/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf725134afb45eda1da4dafbb693a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset flores_101 (/gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/eng/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fb307cacda433aa93c57b0306414e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset flores_101 (/gscratch/zlab/ahai/hf/datasets/gsarti___flores_101/eng/1.0.0/e663cc717b274f2cef5142df786973abbf1966a7115d99509c062936d77d4335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3608f08e12344061a549fb5e7ae89eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src = load_dataset(\"gsarti/flores_101\", 'zho_simpl')\n",
    "tgt = load_dataset(\"gsarti/flores_101\", 'eng')\n",
    "eng = load_dataset(\"gsarti/flores_101\", 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fa59a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = '中文'\n",
    "tgt_lang = 'English'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d1befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "  ids = []\n",
    "  i = 0\n",
    "  t = 0\n",
    "  while t < len(s):\n",
    "    c = s[t]\n",
    "    if c == '\\n':\n",
    "      ids += s_ids(s[i:t]) + [2]\n",
    "      i = t+1\n",
    "      t = t+1\n",
    "    else:\n",
    "      t += 1\n",
    "  ids += s_ids(s[i:])\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a27eb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_ids(s):\n",
    "  return tokenizer(s).input_ids[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eaa9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demo_prompt(src_endonym, tgt_endonym, src, tgt, k):\n",
    "  src_points = random.sample(list(src), k=k)\n",
    "  tgt_points = [ tgt[point['id']-1] for point in src_points ]\n",
    "  demos = []\n",
    "  for s, t in zip(src_points, tgt_points):\n",
    "    demo = f\"{src_endonym} : {s['sentence']} {tgt_endonym} : {t['sentence']}\"\n",
    "    demos.append(demo)\n",
    "  demo_prompt_ids = [2]\n",
    "  for demo in demos:\n",
    "    demo_prompt_ids.extend(s_ids(demo))\n",
    "    demo_prompt_ids.append(2)\n",
    "  return demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dc4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b40acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6c019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dd5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63953d94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util.mt' from '/mmfs1/gscratch/zlab/ahai/repo/infogain/util/mt.py'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48c418ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_points = random.sample(list(src['dev']), k=10)\n",
    "tgt_points = [ tgt['dev'][point['id']-1] for point in src_points ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d891d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_prompt_ids = mt.make_demo_prompt(src_points, tgt_points, src_lang, tgt_lang, tokenizer)\n",
    "uncond_demo_prompt_ids = mt.make_uncond_demo_prompt(tgt_points, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a65d8bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "伊尔-76 自 20 世纪 70 年代以来，一直是俄罗斯和苏联军队的主要组成部分，上个月在俄罗斯已经发生过一起严重事故。\n",
      "\n",
      "The Il-76 has been a major component of both the Russian and Soviet military since the 1970s, and had already seen a serious accident in Russia last month.\n",
      "\n",
      "The Il-76 has been a major component of both the Russian and Soviet military since the 1970s, and had already seen a serious accident in Russia last month.\n"
     ]
    }
   ],
   "source": [
    "idx += 1\n",
    "inference_point = src['dev'][idx] \n",
    "cond_prompt_ids = demo_prompt_ids + s_ids(f\"{src_lang} : {inference_point['sentence']} {tgt_lang} :\")\n",
    "cond_prompt_t = torch.LongTensor([cond_prompt_ids]).to(model.device)\n",
    "uncond_prompt_ids = demo_prompt_ids + s_ids(f\"{tgt_lang} :\")\n",
    "uncond_prompt_t = torch.LongTensor([uncond_prompt_ids]).to(model.device)\n",
    "print(src['dev'][idx]['sentence'])\n",
    "print()\n",
    "print(tgt['dev'][idx]['sentence'])\n",
    "print()\n",
    "print(eng['dev'][idx]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f115a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Il-76 has been in service since the 1970s, and in Russia last month there was a serious accident.</s> 中文 : 他们在那里呆了两个星期,然后他们就离开了。 English : They stayed there for two\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  gen_ids = mt.cfg_k_greedy(model, 0.3, cond_prompt_t, uncond_prompt_t, 50, k=3)\n",
    "print(tokenizer.decode(gen_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "237ad239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Il-76 has been in service since the 1970s, and it has been involved in several serious accidents in Russia.</s> 中文 : 他们在那里呆了两个星期,然后他们就离开了。 English : They stayed there for\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  greedy_gen_ids = mt.greedy(model, cond_prompt_t, 50)\n",
    "print(tokenizer.decode(greedy_gen_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b7d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "956affb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s> 中文 : 퓨어랜드 종이접기는 한 번에 접기는 한 번만 해야 하고, 더 복잡한 뒤집어 접기는 허용되지 않으며 모든 접기는 단순한 위치여야 한다는 제한이 있는 종이접기 방식이다. English : Pureland-origamiin liittyy sellaisia rajoituksia, että kerrallaan voidaan tehdä vain yksi taitos, monimutkaisemmat taitokset, kuten käänteiset taitokset, ovat kiellettyjä, ja kaikilla taitoksilla on suoraviivainen paikka.</s> 中文 : \"전 세계 대부분에선 손을 흔드는 것은 \"\"안녕하세요\"\"를 나타내는 친절한 제스처입니다.\" English : Monissa osissa maailmaa vilkuttaminen on ystävällinen ele, joka merkitsee tervehtimistä.</s> 中文 : 이 분야의 애플리케이션 중 일부는 이용자가 스마트폰을 대상을 향하게 하면, 표지판 등에 적힌 외국어 텍스트를 번역하기도 한다. English : Jotkin tähän luokkaan kuuluvat sovellukset pystyvät jopa kääntämään vieraskielistä tekstiä kylteistä tai muista reaalimaailman kohteista, kun käyttäjä osoittaa niihin älypuhelimellaan.</s> 中文 : 사막의 한더위도 밤에는 <unk>시 매서운 추위로 변할 수 있다. 따뜻한 옷 없이 저체온증은 실질적인 위험이다. English : Jopa kuumimmat autiomaat voivat olla äärimmäisen kylmiä yöllä. Hypotermia on todellinen riski ilman lämpimiä vaatteita.</s> 中文 : 사유지나 여느 도시에 텐트를 설치하는 것은 원치 않는 주목을 쉽게 받을 수 있습니다. English : Teltan pystyttäminen yksityisalueelle tai minkä kokoiseen kaupunkiin tahansa voi helposti vetää puoleensa ei-toivottua huomiota.</s> 中文 : 만일 추가적으로 여행을 할 수 있는 시간이 있다면 아프리카 여행경비와 세계 경 주비용을 비교해보십시오. English : Jos sinulla on ylimääräistä matka-aikaa, kannattaa tarkistaa, kuinka Afrikan matkan kokonaishinta vertautuu maailmanympärysmatkan hintaan.</s> 中文 : 재판은 버밍엄 크라운 법원에서 열렸으며 8월 3일에 끝났습니다. English : Oikeudenkäynti järjestettiin Birminghamin rikostuomioistuimessa ja päättyi 3. elokuuta.</s> 中文 : 도미니카 공화국(스페인어 명: República Dominicana)은 히스파니올라 섬의 동부 지역을 차지하고 있는데, 이 땅을 아이티와 공유하고 있습니다. English : Dominikaaninen tasavalta (espanjaksi República Dominicana) on Karibianmeren maa, joka muodostaa itäpuolen Haitin kanssa jakamastaan Hispaniolan saaresta.</s> 中文 : 그는 일요일의 볼로냐전에 앞서서 팀 숙소인 호텔에서 지내고 있었다. English : Hän oli majoittunut joukkueen hotelliin ennen sunnuntaille aiottua ottelua Boloniaa vastaan.</s> 中文 : 1960년대에 그는 영화 연출을 교육하고자 새롭게 독립한 알제리아로 돌아갔다. English : 1960-luvulla hän suuntasi takaisin vasta itsenäistyneeseen Algeriaan opettamaan elokuvien ohjausta.</s> 中文 : 飞机是飞往伊尔库茨克(Irkutsk)的,并且由内政部队负责驾驶。 English :'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(cond_prompt_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "3a85bd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   218,   2686,    113,     88,    256,  14170,    753,     48,     32,\n",
       "           1075,    316,     20,     67,     11,  19282,  16740,     22,     32,\n",
       "         122605,      5,      2,  73913,    129,    218,     25, 168217,    395,\n",
       "            824,    102,  54862,    522,    247,  11076,  66092, 141249,  40165,\n",
       "             64,   2494,    476,    218,     25,  46298,   5281,     27,    102,\n",
       "           2774,      5,   7614,    129,    218]], device='cuda:0')"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f3663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91c4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34124290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a1cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227260e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633ce8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88858c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "d1b7ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = torch.LongTensor([tokenizer('translate into english, 中文： 谢谢你！').input_ids[:-1]]).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "93d7c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ids = torch.LongTensor([tokenize('中文： 谢谢你! english:')]).to(model.device)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0602b35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 19773, 13, 6, 13305, 13305, 710, 35, 83763, 13]]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "fe912cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGLMConfig {\n",
       "  \"_name_or_path\": \"facebook/xglm-2.9B\",\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"XGLMForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"attention_heads\": 16,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"d_model\": 2048,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"ffn_dim\": 8192,\n",
       "  \"init_std\": 0.02,\n",
       "  \"layerdrop\": 0.0,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"xglm\",\n",
       "  \"num_layers\": 48,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": true,\n",
       "  \"transformers_version\": \"4.22.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256008\n",
       "}"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "cdd568ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52376/3632386642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mgen_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mmfs1/gscratch/zlab/ahai/repo/infogain/util/mt.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(model, prompt_ids, length, temp)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgen_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcur_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  gen_ids = mt.sample(model, prompt_ids, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "39940ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leading researchers say that the test can detect cancer, tuberculosis, HIV and malaria in patients who live in low-income countries, where for example, the survival rate of breast cancer is half that of the richer countries.</s> Finnish.</s> Finnish : Hän sanoi, että hän sanoi, että hän sanoi, että, että, että, että, että, että, että, että, että, että, että, että, että, että, että, että,'"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "25d4f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文: 谢谢你! | english:\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b70b758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stanfordin yliopiston lääketieteen laitoksen tutkijat ilmoittivat maanantaina uuden diagnostiikkatyökalun keksimisestä: solut tyypin mukaan lajitteleva pienenpieni tulostettava siru, joka voidaan valmistaa normaaleilla mustesuihkutulostimilla mahdollisesti noin yhden Yhdysvaltain sentin kappalehintaan.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_point['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9a74a4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_ids('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e9f6fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "  ids = []\n",
    "  i = 0\n",
    "  t = 0\n",
    "  while t < len(s):\n",
    "    c = s[t]\n",
    "    if c == '\\n':\n",
    "      ids += s_ids(s[i:t]) + [2]\n",
    "      i = t+1\n",
    "      t = t+1\n",
    "    else:\n",
    "      t += 1\n",
    "  ids += s_ids(s[i:])\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8169866",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "tokens = list(sorted(tokenizer.get_vocab(), key=lambda k: vocab[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c9b91f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<pad>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " ',',\n",
       " '.',\n",
       " '▁',\n",
       " 's',\n",
       " '-',\n",
       " 'a',\n",
       " '▁de',\n",
       " '▁a',\n",
       " 'e',\n",
       " ':',\n",
       " 'i']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c9b43d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_madeup_words',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_file',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "253e0e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "5f05feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_demo_prompt(src_points, tgt_points, src_name, tgt_name, src, tgt, k, tokenizer):\n",
    "    demos = []\n",
    "    for s, t in zip(src_points, tgt_points):\n",
    "        assert(s['id'] == t['id'])\n",
    "        demo = f\"{src_name} : {s['sentence']} {tgt_name} : {t['sentence']}\"\n",
    "        demos.append(demo)\n",
    "    demo_prompt_ids = [2]\n",
    "    for demo in demos:\n",
    "      demo_prompt_ids.extend(mt.xglm_tokenize(demo, tokenizer))\n",
    "      demo_prompt_ids.append(2)\n",
    "    return demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c629e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_points = random.sample(list(src['dev']), k=3)\n",
    "tgt_points = [ tgt['dev'][point['id']-1] for point in src_points ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4737f0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util.mt' from '/mmfs1/gscratch/zlab/ahai/repo/infogain/util/mt.py'>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "563a6b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " finnish : Kiertoajelut ovat halvempia suurille ryhmille, joten jos liikut yksin tai vain yhden ystävän kanssa, koeta tutustua muihin ihmisiin ja muodostaa 4–6 hengen ryhmä parempaa yksilöhintaa varten. english : Tours are cheaper for larger groups, so if you're by yourself or with just one friend, try to meet other people and form a group of four to six for a better per-person rate.\n",
      " finnish : Vaatimukset on tarkoitettu tekemään maiden välisestä muuttovirrasta järjestelmällisen. english : These requirements are designed to provide an organized migratory flow between both countries.\n",
      " finnish : Varo: pikkukaupunkien baarit eivät täälläpäin aina ole parhaita ajanviettopaikkoja vieraspaikkakuntalaisille. english : Beware: small-town bars here are not always good places for the out-of-state visitor to hang out.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mt.post_proc(tokenizer.decode(mt.make_demo_prompt(src_points, tgt_points, 'finnish', 'english', tokenizer))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "2ccd9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uncond_demo_prompt(tgt_points, tokenizer):\n",
    "    uncond_demo_prompt_ids = [2]\n",
    "    for t in tgt_points:\n",
    "      uncond_demo_prompt_ids.extend(mt.xglm_tokenize(t['sentence'], tokenizer))\n",
    "      uncond_demo_prompt_ids.append(2)\n",
    "    return uncond_demo_prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "09b18b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tours are cheaper for larger groups, so if you're by yourself or with just one friend, try to meet other people and form a group of four to six for a better per-person rate.\n",
      " These requirements are designed to provide an organized migratory flow between both countries.\n",
      " Beware: small-town bars here are not always good places for the out-of-state visitor to hang out.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mt.post_proc(tokenizer.decode(make_uncond_demo_prompt(tgt_points, tokenizer))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168d750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
